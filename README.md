# realistic-paper
## GCD
Learning Semi-supervised Gaussian Mixture Models for Generalized Category Discovery [[2023](https://arxiv.org/pdf/2305.06144)]  
Targeted Representation Alignment for Open-World Semi-Supervised Learning [[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Targeted_Representation_Alignment_for_Open-World_Semi-Supervised_Learning_CVPR_2024_paper.pdf)]  
Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery [[2024](https://arxiv.org/pdf/2403.09974)]  
OwMatch: Conditional Self-Labeling with Consistency for Open-World Semi-Supervised Learning [[2024](https://arxiv.org/pdf/2411.01833)]  
multimodal generalized category discovery [[2024](https://arxiv.org/pdf/2409.11624)]  
* Learning to Distinguish Samples for Generalized Category Discovery[[2024 ECCV](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08177.pdf)]

## ActiveLearning
Margin-Based Active Learning for Structured Output Spaces [[2006](https://link.springer.com/chapter/10.1007/11871842_40)]  
ACTIVE LEARNING FOR CONVOLUTIONAL NEURAL NETWORKS: A CORE-SET APPROACH [[2018](https://arxiv.org/pdf/1708.00489)]  
Learning Loss for Active Learning [[2019](https://arxiv.org/pdf/1905.03677)]  
DEEP BATCH ACTIVE LEARNING BY DIVERSE, UNCERTAIN GRADIENT LOWER BOUNDS [[ICLR2020](https://arxiv.org/pdf/1906.03671)]  
Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm [[2023 CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Active_Finetuning_Exploiting_Annotation_Budget_in_the_Pretraining-Finetuning_Paradigm_CVPR_2023_paper.pdf)]  
Boundary Matters: A Bi-Level Active Finetuning Framework [[2024](https://arxiv.org/pdf/2403.10069)]  
ActiveDC: Distribution Calibration for Active Finetuning [[CVPR 2024](https://arxiv.org/pdf/2311.07634)][[code](https://github.com/VincentXu521/ActiveDC/tree/master)]  
Active Generalized Category Discovery [[2024 CVPR](https://arxiv.org/pdf/2403.04272)]  
Active Prompt Learning in Vision Language Models [[2024 CVPR](https://arxiv.org/pdf/2311.11178v3)]  
Active Prompt Learning with Vision-Language Model Priors [[2024](https://arxiv.org/pdf/2411.16722)]

## Label Selection
Towards Free Data Selection with General-Purpose Models  [[2023 NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2023/file/047682108c3b053c61ad2da5a6057b4e-Paper-Conference.pdf)]  
Labeled Data Selection for Category Discovery [[2024 ECCV](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07212.pdf)]  
a clip-power framework for robust and generalizable data selection [[2024](https://arxiv.org/pdf/2410.11215)]

## Multimodal
Connecting Multi-modal Contrastive Representations [[NeurIPS 2023](https://arxiv.org/pdf/2305.14381)]  
VLM2VEC: training vision-language models for massive multimodal embedding tasks [[2024](https://arxiv.org/pdf/2410.05160)]  
CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training [[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_CrossMAE_Cross-Modality_Masked_Autoencoders_for_Region-Aware_Audio-Visual_Pre-Training_CVPR_2024_paper.pdf)]  
Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning  
Extending Multi-modal Contrastive Representations [[NeurIPS 2024](https://arxiv.org/pdf/2310.08884)]  
UMFC: Unsupervised Multi-Domain Feature Calibration for Vision-Language Models [[2024](https://arxiv.org/pdf/2411.06921)]

## zero & few shot
Laplacian Regularized Few-Shot Learning [[2021](https://proceedings.mlr.press/v119/ziko20a/ziko20a.pdf)]   
Parameterless Transductive Feature Re-representation for Few-Shot Learning [[ICML 2021](https://proceedings.mlr.press/v139/cui21a/cui21a.pdf)]  
Class-Aware Patch Embedding Adaptation for Few-Shot Image Classification 
[[ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Hao_Class-Aware_Patch_Embedding_Adaptation_for_Few-Shot_Image_Classification_ICCV_2023_paper.pdf)]  
Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement [[2023 CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Transductive_Few-Shot_Learning_With_Prototype-Based_Label_Propagation_by_Iterative_Graph_CVPR_2023_paper.pdf)]  
Transductive Zero-Shot and Few-Shot CLIP [[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Martin_Transductive_Zero-Shot_and_Few-Shot_CLIP_CVPR_2024_paper.pdf)]
Boosting Vision-Language Models with Transduction [[ 2024 ](https://arxiv.org/pdf/2406.01837)]  
Selective Vision-Language Subspace Projection for Few-shot CLIP [[2024](https://arxiv.org/pdf/2407.16977)]  
Interpreting and Analyzing CLIPâ€™s Zero-Shot Image Classification via Mutual Knowledge [[2024](https://arxiv.org/pdf/2410.13016)]  
UNEM: UNrolled Generalized EM for Transductive Few-Shot Learning [[2024](https://arxiv.org/pdf/2412.16739)]

## Model Merging
EDITING MODELS WITH TASK ARITHMETIC [[ICLR 2023](https://arxiv.org/pdf/2212.04089)]  
Parameter Competition Balancing for Model Merging [[NeurIPS 2024](https://arxiv.org/pdf/2410.02396)]  
BRAVE: Broadening the visual encoding of vision-language models [[ECCV 2024](https://brave-vlms.epfl.ch/)]  
EMR-MERGING: Tuning-Free High-Performance Model Merging [[ 2024 ](https://arxiv.org/pdf/2405.17461)]

## Training Methods
All4One: Symbiotic Neighbour Contrastive Learning via Self-Attention and Redundancy Reduction [[ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Estepa_All4One_Symbiotic_Neighbour_Contrastive_Learning_via_Self-Attention_and_Redundancy_Reduction_ICCV_2023_paper.pdf)]  
A Unified Contrastive Loss for Self-Training [[2024](https://arxiv.org/pdf/2409.07292)]  
vision transformers need registers VISION [[2024 ICLR](https://arxiv.org/pdf/2309.16588)]  

## Unsupervised Adaptation
DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models [[2024](https://arxiv.org/pdf/2408.08855)]  

## Representation Learning
TPTE: Text-guided Patch Token Exploitation for Unsupervised Fine-Grained Representation Learning [[ACM Trans](https://dl.acm.org/doi/pdf/10.1145/3673657)]  
Disentangled Representation Learning [[2024](https://arxiv.org/pdf/2211.11695)]  

## Instance Retrieval
Cluster-Aware Similarity Diffusion for Instance Retrieval [[2024](https://arxiv.org/pdf/2406.02343)]

## semantic segmentation
Vision Transformers for Dense Prediction (DPT)[[2021 ICCV](https://arxiv.org/abs/2103.13413v1)]  
Language-driven Semantic Segmentation [[2022 ICLR](https://arxiv.org/pdf/2201.03546)]  
Extract Free Dense Labels from CLIP (MASK CLIP) [[2022 ECCV](https://arxiv.org/pdf/2112.01071)]   
ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation [[2023 CVPR](https://arxiv.org/pdf/2212.03588)]  
Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation [[2024 ECCV](https://arxiv.org/pdf/2407.08268)]  
ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation [[ECCV](https://arxiv.org/pdf/2408.04883)]  
SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference [[ECCV 2024](https://arxiv.org/pdf/2312.01597)]  
Revisit Anything: Visual Place Recognition via Image Segment Retrieval [[2024](https://arxiv.org/pdf/2409.18049)]  
Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic Segmentation [[2024](https://arxiv.org/pdf/2404.08181)]  
ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference[[ECCV 2024](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06346.pdf)]  
BRIDGING THE GAP TO REAL-WORLD OBJECTCENTRIC LEARNING [[2023](https://arxiv.org/pdf/2209.14860)]  
DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment [[2024](https://arxiv.org/pdf/2412.16334)]



## Clustering
Clustering and Projected Clustering with Adaptive Neighbors [[2014 KDD](https://matlabtools.com/wp-content/uploads/p1004.pdf)]  
The Constrained Laplacian Rank Algorithm for Graph-Based Clustering [[2016 AAAI](https://ojs.aaai.org/index.php/AAAI/article/download/10302/10161)]  
TOWARDS CALIBRATED DEEP CLUSTERING NETWORK[[2025](https://openreview.net/pdf?id=JvH4jDDcG3)]  

## Recommendation Systems
code[[1](https://github.com/recommenders-team/recommenders?tab=readme-ov-file)] [[2](https://github.com/AmazingDD/daisyRec)]  
DaisyRec 2.0: Benchmarking Recommendation for Rigorous Evaluation [[2022](https://arxiv.org/pdf/2206.10848)]

## temporarily store
Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning  
Interactive Deep Clustering via Value Mining  

## Optimal Transportation
Unsupervised Learning of Visual Features by Contrasting Cluster Assignments [[NeurIPS 2020](https://arxiv.org/pdf/2006.09882)]  
Towards Interpretable Deep Metric Learning with Structural Matching [[ICCV 2021](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Towards_Interpretable_Deep_Metric_Learning_With_Structural_Matching_ICCV_2021_paper.pdf)]  
Optimal Transport Aggregation for Visual Place Recognition [[CVPR 2024](https://arxiv.org/abs/2311.15937)]  
Data Selection via Optimal Control for Language Models [[]()]

## TTA
Parameter-Free_Online_Test-Time_Adaptation [[CVPR 2022](https://openaccess.thecvf.com/content/CVPR2022/papers/Boudiaf_Parameter-Free_Online_Test-Time_Adaptation_CVPR_2022_paper.pdf)]  
Efficient_Test-Time_Adaptation_of_Vision-Language_Models [[CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Karmanov_Efficient_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2024_paper.pdf)]  
Improved_Self-Training_for_Test-Time_Adaptation [[CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_Improved_Self-Training_for_Test-Time_Adaptation_CVPR_2024_paper.pdf)]  

## Visual Place Recognition
NetVLAD: CNNarchitecture for weakly supervised place recognition [[CVPR 2016](https://openaccess.thecvf.com/content_cvpr_2016/papers/Arandjelovic_NetVLAD_CNN_Architecture_CVPR_2016_paper.pdf)]  
Optimal Transport Aggregation for Visual Place Recognition [[CVPR 2024](https://arxiv.org/pdf/2311.15937)]   

## interpretable
SPARSE AUTOENCODERS FIND HIGHLY INTERPRETABLE FEATURES IN LANGUAGE MODELS [[2024](https://arxiv.org/pdf/2309.08600)]  
Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE ) [[2024](https://arxiv.org/pdf/2402.10376)]  
ENHANCING PRE-TRAINED REPRESENTATION CLASSIFIABILITY CAN BOOST ITS INTERPRETABILITY [[under review 2025 ICLR](https://openreview.net/pdf?id=GjfIZan5jN)]  
A SIMPLE INTERPRETABLE TRANSFORMER FOR FINEGRAINED IMAGE CLASSIFICATION AND ANALYSIS [[ICLR 2024](https://arxiv.org/pdf/2311.04157)]

## Action Recognition
Part-aware Unified Representation of Language and Skeleton for Zero-shot Action Recognition [[CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhu_Part-aware_Unified_Representation_of_Language_and_Skeleton_for_Zero-shot_Action_CVPR_2024_paper.pdf)]

## improve Vit & Patch
Differentiable Patch Selection for Image Recognition [[CVPR 2021](https://openaccess.thecvf.com/content/CVPR2021/papers/Cordonnier_Differentiable_Patch_Selection_for_Image_Recognition_CVPR_2021_paper.pdf)]  
Less is More: Focus Attention for Efficient DETR [[ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Less_is_More_Focus_Attention_for_Efficient_DETR_ICCV_2023_paper.pdf)]  
ITERATIVE PATCH SELECTION FOR HIGH-RESOLUTION IMAGE RECOGNITION [[ICLR 2023](https://arxiv.org/pdf/2210.13007)]  
IdleViT: Efficient Vision Transformer via Token Idle and Token Cut Loss [[AJCAI23](https://arxiv.org/pdf/2310.05654)]  
Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers [[NeurIPS 2024](https://openreview.net/pdf?id=pVPyCgXv57)]  
Token Cropr: Faster ViTs for Quite a Few Tasks [[2024](https://arxiv.org/pdf/2412.00965)]  
Can We Get Rid of Handcrafted Feature Extractors? SparseViT: Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization through Spare-Coding Transformer [[2024](https://arxiv.org/pdf/2412.14598)]  
Learning to Rank Patches for Unbiased Image Redundancy Reduction [[CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_Learning_to_Rank_Patches_for_Unbiased_Image_Redundancy_Reduction_CVPR_2024_paper.pdf)]  
Linguistic-Aware Patch Slimming Framework for Fine-grained Cross-Modal Alignment [[CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Fu_Linguistic-Aware_Patch_Slimming_Framework_for_Fine-grained_Cross-Modal_Alignment_CVPR_2024_paper.pdf)]  

## ReID
A Pedestrian is Worth One Prompt: Towards Language Guidance Person Re-Identification [[CVPR 2024](https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_A_Pedestrian_is_Worth_One_Prompt_Towards_Language_Guidance_Person_CVPR_2024_paper.pdf)]  

## Affective Computing
Most Important Person-guided Dual-branch Cross-Patch Attention for Group Affect Recognition [[ICCV 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_Most_Important_Person-Guided_Dual-Branch_Cross-Patch_Attention_for_Group_Affect_Recognition_ICCV_2023_paper.pdf)]  
