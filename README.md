# Content
[[GCD](#GCD)]  [[Patch Selection](#Patch-Selection)]  [[ActiveLearning](#ActiveLearning)] [[Multimodal](#Multimodal)] 

-----------------------------------------------------------------------------------------------
# GCD
[[2023 I/ECCV](https://arxiv.org/pdf/2305.06144)] Learning Semi-supervised Gaussian Mixture Models for Generalized Category Discovery    
[[2023 NeruIPS](https://proceedings.neurips.cc/paper_files/paper/2023/file/3f52ab4322e967efd312c38a68d07f01-Paper-Conference.pdf)]No Representation Rules Them All in Category Discovery   
[[2024 I/ECCV](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08177.pdf)] Learning to Distinguish Samples for Generalized Category Discovery  
[[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Targeted_Representation_Alignment_for_Open-World_Semi-Supervised_Learning_CVPR_2024_paper.pdf)] Targeted Representation Alignment for Open-World Semi-Supervised Learning   
[[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Fu_Linguistic-Aware_Patch_Slimming_Framework_for_Fine-grained_Cross-Modal_Alignment_CVPR_2024_paper.pdf)] Linguistic-Aware Patch Slimming Framework for Fine-grained Cross-Modal Alignment   
[[2024 NeruIPS](https://arxiv.org/pdf/2411.01833)] OwMatch: Conditional Self-Labeling with Consistency for Open-World Semi-Supervised Learning    
[[2025 ICLR](https://openreview.net/pdf?id=On8E0U9vbz)] GENERALIZED CATEGORY DISCOVERY UTILIZING RECIPROCAL LEARNING AND CLASS-WISE DISTRIBUTION REGULARIZATION   
[[2024 ArXiv](https://arxiv.org/pdf/2409.11624)] Multimodal Generalized Category Discovery   
[[2025 CVPR](https://arxiv.org/pdf/2403.09974)] Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery   
[[2025 CVPR](https://arxiv.org/abs/2503.12035)] MOS: Modeling Object-Scene Associations in Generalized Category Discovery (CVPR 2025)  
[[2025 ArXiv](https://arxiv.org/pdf/2502.09501)] Prior-Constrained Association Learning for Fine-Grained Generalized Category Discovery  
[[2025 ArXiv](https://arxiv.org/pdf/2503.16782)] Learning Part Knowledge to Facilitate Category Understanding for Fine-Grained Generalized Category Discovery  
[[2025 ArXiv](https://arxiv.org/pdf/2502.09501)] Prior-Constrained Association Learning for Fine-Grained Generalized Category Discovery   
[[2025 Arxiv](https://arxiv.org/pdf/2503.14500)] Utilization of Neighbor Information for Image Classification with Different Levels of Supervision  
[[2025 Arixv](https://openreview.net/pdf?id=On8E0U9vbz)] GENERALIZED CATEGORY DISCOVERY UTILIZING RECIPROCAL LEARNING AND CLASS-WISE DISTRIBUTION REGULARIZATION   
[[*2025 CVPR]()]  Less Attention is More: Prompt Transformer for Generalized Category Discovery    

-----------------------------------------------------------------------------------------------
# Patch Selection
[[2021 CVPR](https://openaccess.thecvf.com/content/CVPR2021/papers/Cordonnier_Differentiable_Patch_Selection_for_Image_Recognition_CVPR_2021_paper.pdf)] Differentiable Patch Selection for Image Recognition   
[[2022 CVPR](https://arxiv.org/pdf/2112.07658)] A-ViT: Adaptive Tokens for Efficient Vision Transformer    
[[2022 ICLR](https://arxiv.org/pdf/2202.07800)] EVit:NOT ALL PATCHES ARE WHAT YOU NEED:EXPEDITING VISION TRANSFORMERS VIA TOKEN REORGANIZATIONS   
[[2023 ICLR](https://arxiv.org/pdf/2210.13007)] ITERATIVE PATCH SELECTION FOR HIGH-RESOLUTION IMAGE RECOGNITION   
[[2023 ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Less_is_More_Focus_Attention_for_Efficient_DETR_ICCV_2023_paper.pdf)] Less is More: Focus Attention for Efficient DETR   
[[2023 AJCAI](https://arxiv.org/abs/2310.05654)] IdleViT: Efficient Vision Transformer via Token Idle and Token Cut Loss    
[[2024 ArXiv](https://arxiv.org/pdf/2412.00965)] Token Cropr: Faster ViTs for Quite a Few Tasks    
[[2024 AAAI](https://arxiv.org/pdf/2412.14598)] Can We Get Rid of Handcrafted Feature Extractors? SparseViT: Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization through Spare-Coding Transformer   
[[2024 NeurIPS](https://openreview.net/pdf?id=pVPyCgXv57)] Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers   
[[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Luo_Learning_to_Rank_Patches_for_Unbiased_Image_Redundancy_Reduction_CVPR_2024_paper.pdf)] Learning to Rank Patches for Unbiased Image Redundancy Reduction   
[[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Fu_Linguistic-Aware_Patch_Slimming_Framework_for_Fine-grained_Cross-Modal_Alignment_CVPR_2024_paper.pdf)] Linguistic-Aware Patch Slimming Framework for Fine-grained Cross-Modal Alignment   
[[2025 ICLR](https://arxiv.org/pdf/2502.18137)] SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference     
[[*2025]()] No Pains, More Gains: Recycling Sub-Salient Patches for Efficient High-Resolution Image Recognition  

-----------------------------------------------------------------------------------------------
# Large Model Finetune & zero/few-shot
[[2020 ICML](https://proceedings.mlr.press/v119/ziko20a/ziko20a.pdf)] Laplacian Regularized Few-Shot Learning     
[[2021 ICML](https://proceedings.mlr.press/v139/cui21a/cui21a.pdf)] Parameterless Transductive Feature Re-representation for Few-Shot Learning   
[[2023_ICCV](https://openaccess.thecvf.com/content/ICCV2023/papers/Hao_Class-Aware_Patch_Embedding_Adaptation_for_Few-Shot_Image_Classification_ICCV_2023_paper.pdf)] Class-Aware Patch Embedding Adaptation for Few-Shot Image Classification   
[[2023 CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Transductive_Few-Shot_Learning_With_Prototype-Based_Label_Propagation_by_Iterative_Graph_CVPR_2023_paper.pdf)] Transductive Few-shot Learning with Prototype-based Label Propagation by Iterative Graph Refinement   
[[2023 NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2023/file/bf85879363044ca21f7868a3d1b4021c-Paper-Conference.pdf)] Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning  
[[2023 NeurIPS](https://arxiv.org/pdf/2305.18287)] LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections    
[[2024 ICML](https://arxiv.org/pdf/2406.10502)] Candidate Pseudolabel Learning   
[[2024 NeurIPS](https://arxiv.org/pdf/2406.01837)] Boosting Vision-Language Models with Transduction   
[[2024](https://arxiv.org/pdf/2407.16977)] Selective Vision-Language Subspace Projection for Few-shot CLIP    
[[2024 NeurIPS](https://arxiv.org/pdf/2410.13016)] Interpreting and Analyzing CLIPâ€™s Zero-Shot Image Classification via Mutual Knowledge    
[[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Martin_Transductive_Zero-Shot_and_Few-Shot_CLIP_CVPR_2024_paper.pdf)] Transductive Zero-Shot and Few-Shot CLIP    
[[2024-NIPS](https://arxiv.org/pdf/2404.08461)] OTTER: Effortless Label Distribution Adaptation of Zero-shot Models   
[[2025 IETIP](https://ieeexplore.ieee.org/abstract/document/10925517)] Task-to-Instance Prompt Learning for Vision-Language Models at Test Time     
[[2025 CVPR](https://arxiv.org/pdf/2412.16739)] UNEM: UNrolled Generalized EM for Transductive Few-Shot Learning     
[[*2025 CVPR]()] Surrogate Prompt Learning: Towards Efficient and Diverse Prompt Learning for Vision-Language Models   
[[*2025 ICML]()] Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models   
[[*2025 ICML]()] From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection  

-----------------------------------------------------------------------------------------------
# ActiveLearning

[[2006](https://link.springer.com/chapter/10.1007/11871842_40)] Margin-Based Active Learning for Structured Output Spaces   
[[2018](https://arxiv.org/pdf/1708.00489)] ACTIVE LEARNING FOR CONVOLUTIONAL NEURAL NETWORKS: A CORE-SET APPROACH   
[[2019](https://arxiv.org/pdf/1905.03677)] Learning Loss for Active Learning   
[[ICLR 2020](https://arxiv.org/pdf/1906.03671)] DEEP BATCH ACTIVE LEARNING BY DIVERSE, UNCERTAIN GRADIENT LOWER BOUNDS   
[[2023 CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Active_Finetuning_Exploiting_Annotation_Budget_in_the_Pretraining-Finetuning_Paradigm_CVPR_2023_paper.pdf)] Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm      
[[2023 NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2023/file/047682108c3b053c61ad2da5a6057b4e-Paper-Conference.pdf)] Towards Free Data Selection with General-Purpose Models    
[[2024](https://arxiv.org/pdf/2403.10069)] Boundary Matters: A Bi-Level Active Finetuning Framework   
[[2024 CVPR](https://arxiv.org/pdf/2311.07634)][[code](https://github.com/VincentXu521/ActiveDC/tree/master)] ActiveDC: Distribution Calibration for Active Finetuning   
[[2024 CVPR](https://arxiv.org/pdf/2403.04272)] Active Generalized Category Discovery   
[[2024 CVPR](https://arxiv.org/pdf/2311.11178v3)] Active Prompt Learning in Vision Language Models   
[[2024 Arxiv](https://arxiv.org/pdf/2411.16722)] Active Prompt Learning with Vision-Language Model Priors
[[2024 ECCV](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07212.pdf)] Labeled Data Selection for Category Discovery      
[[2024 Arxiv](https://arxiv.org/pdf/2410.11215)] a clip-power framework for robust and generalizable data selection   
[[2025 ICLR](https://arxiv.org/pdf/2406.04273)] ELFS: LABEL-FREE CORESET SELECTION WITH PROXY TRAINING DYNAMICS   
[[2025 Arxiv](https://arxiv.org/pdf/2502.08227)] Enhancing Sample Selection by Cutting Mislabeled Easy Examples    
[[*2025 ICML]()] Foundation Model insights and a Multi-model Approach for Superior Fine-grained One-shot Subset Selection   
[[2025 ArXiv](https://arxiv.org/pdf/2412.00420)] TAROT- Targeted Data Selection via Optimal Transport   

-----------------------------------------------------------------------------------------------
# Multimodal
[[2023](https://arxiv.org/pdf/2308.12966)] Qwen-VL: AVersatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond    
[[2023](http://arxiv.org/abs/2311.04257)] mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration   
[[2023 NeurIPS](https://arxiv.org/pdf/2305.14381)] Connecting Multi-modal Contrastive Representations   
[[2024](https://arxiv.org/pdf/2410.05160)] VLM2VEC: training vision-language models for massive multimodal embedding tasks   
[[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_CrossMAE_Cross-Modality_Masked_Autoencoders_for_Region-Aware_Audio-Visual_Pre-Training_CVPR_2024_paper.pdf)] CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training   
[[2024 NeurIPS](https://arxiv.org/pdf/2411.03978?)] Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning  
[[2024 NeurIPS](https://arxiv.org/pdf/2310.08884)] Extending Multi-modal Contrastive Representations    
[[2024](https://arxiv.org/pdf/2411.06921)] UMFC: Unsupervised Multi-Domain Feature Calibration for Vision-Language Models   
[[2024](https://arxiv.org/pdf/2408.08855)] DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models  
[[2024 CVPR H](https://arxiv.org/pdf/2311.06607)] ImageResolutionandText Label Are Important Things for Large Multi-modal Models    
[[2025 ArXiv](https://arxiv.org/pdf/2504.12717)] Post-pre-training for Modality Alignment in Vision-Language Foundation Models  

-----------------------------------------------------------------------------------------------
# Writing
[[2025 ArXiv](https://arxiv.org/pdf/2505.02056)] Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin   
[[CVPR 2025 Highlight](http://arxiv.org/abs/2504.03193)] Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation  
