# realistic-paper
## GCD
Targeted Representation Alignment for Open-World Semi-Supervised Learning[[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Xiao_Targeted_Representation_Alignment_for_Open-World_Semi-Supervised_Learning_CVPR_2024_paper.pdf)]  
Learning Semi-supervised Gaussian Mixture Models for Generalized Category Discovery [[2023](https://arxiv.org/pdf/2305.06144)]  
Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery [[2024](https://arxiv.org/pdf/2403.09974)]  
* Learning to Distinguish Samples for Generalized Category Discovery[[2024 ECCV](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08177.pdf)]

## ActiveLearning
Margin-Based Active Learning for Structured Output Spaces [[2006](https://link.springer.com/chapter/10.1007/11871842_40)]  
ACTIVE LEARNING FOR CONVOLUTIONAL NEURAL NETWORKS: A CORE-SET APPROACH [[2018](https://arxiv.org/pdf/1708.00489)]  
Learning Loss for Active Learning [[2019](https://arxiv.org/pdf/1905.03677)]  
DEEP BATCH ACTIVE LEARNING BY DIVERSE, UNCERTAIN GRADIENT LOWER BOUNDS [[ICLR2020](https://arxiv.org/pdf/1906.03671)]  
Boundary Matters: A Bi-Level Active Finetuning Framework [[2024](https://arxiv.org/pdf/2403.10069)]  
ActiveDC: Distribution Calibration for Active Finetuning [[CVPR 2024](https://arxiv.org/pdf/2311.07634)][[code](https://github.com/VincentXu521/ActiveDC/tree/master)]  
Active Generalized Category Discovery [[2024 CVPR](https://arxiv.org/pdf/2403.04272)]  
Active Prompt Learning in Vision Language Models [[2024 CVPR](https://arxiv.org/pdf/2311.11178v3)]  
Active Finetuning: Exploiting Annotation Budget in the Pretraining-Finetuning Paradigm [[2023 CVPR](https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Active_Finetuning_Exploiting_Annotation_Budget_in_the_Pretraining-Finetuning_Paradigm_CVPR_2023_paper.pdf)]

## Label Selection
Labeled Data Selection for Category Discovery [[2024 ECCV](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07212.pdf)]  
Towards Free Data Selection with General-Purpose Models  [[2023 NeurIPS](https://proceedings.neurips.cc/paper_files/paper/2023/file/047682108c3b053c61ad2da5a6057b4e-Paper-Conference.pdf)]  
A CLIP-POWERED FRAMEWORK FOR ROBUST AND GENERALIZABLE DATA SELECTION [[2024](https://arxiv.org/pdf/2410.11215)]

## Multimodal
VLM2VEC: TRAINING VISION-LANGUAGE MODELS FOR MASSIVE MULTIMODAL EMBEDDING TASKS [2024](https://arxiv.org/pdf/2410.05160)  
MULTIMODAL GENERALIZED CATEGORY DISCOVERY [2024](https://arxiv.org/pdf/2409.11624)  
CrossMAE: Cross-Modality Masked Autoencoders for Region-Aware Audio-Visual Pre-Training [[2024 CVPR](https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_CrossMAE_Cross-Modality_Masked_Autoencoders_for_Region-Aware_Audio-Visual_Pre-Training_CVPR_2024_paper.pdf)]  
Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning  
Connecting Multi-modal Contrastive Representations [[NeurIPS 2023](https://arxiv.org/pdf/2305.14381)]  
Extending Multi-modal Contrastive Representations [[NeurIPS 2024](https://arxiv.org/pdf/2310.08884)]  
BoostingVision-LanguageModelswithTransduction [[ 2024 ](https://arxiv.org/pdf/2406.01837)]

## Model Merging
Parameter Competition Balancing for Model Merging [NeurIPS 2024](https://arxiv.org/pdf/2410.02396)  
BRAVE: Broadening the visual encoding of vision-language models [[ECCV 2024](https://brave-vlms.epfl.ch/)]


## Training Methods
A Unified Contrastive Loss for Self-Training [2024](https://arxiv.org/pdf/2409.07292)  

## Unsupervised Adaptation
DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models [2024](https://arxiv.org/pdf/2408.08855)  

## Representation Learning
TPTE: Text-guided Patch Token Exploitation for Unsupervised Fine-Grained Representation Learning [ACM Trans](https://dl.acm.org/doi/pdf/10.1145/3673657)  
Disentangled Representation Learning [[2024](https://arxiv.org/pdf/2211.11695)]  

## Vision-language relationship
Selective Vision-Language Subspace Projection for Few-shot CLIP [2024](https://arxiv.org/pdf/2407.16977)  
Interpreting and Analyzing CLIPâ€™s Zero-Shot Image Classification via Mutual Knowledge [[2024](https://arxiv.org/pdf/2410.13016)]

## Instance Retrieval
Cluster-Aware Similarity Diffusion for Instance Retrieval [2024](https://arxiv.org/pdf/2406.02343)

## semantic segmentation
Vision Transformers for Dense Prediction (DPT)[[2021 ICCV](https://arxiv.org/abs/2103.13413v1)]  
Language-driven Semantic Segmentation [[2022 ICLR](https://arxiv.org/pdf/2201.03546)]  
Extract Free Dense Labels from CLIP (MASK CLIP)[[2022 ECCV](https://arxiv.org/pdf/2112.01071)]   
ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation [[2023 CVPR](https://arxiv.org/pdf/2212.03588)]  
Explore the Potential of CLIP for Training-Free Open Vocabulary Semantic Segmentation[[2024 ECCV](https://arxiv.org/pdf/2407.08268)]  
ProxyCLIP: Proxy Attention Improves CLIP for Open-Vocabulary Segmentation[[ECCV](https://arxiv.org/pdf/2408.04883)]  
SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference[[ECCV 2024](https://arxiv.org/pdf/2312.01597)]  
Revisit Anything: Visual Place Recognition via Image Segment Retrieval [[2024](https://arxiv.org/pdf/2409.18049)]

## deep clustering
TOWARDS CALIBRATED DEEP CLUSTERING NETWORK[[2025](https://openreview.net/pdf?id=JvH4jDDcG3)]

## Recommendation Systems
code[[1](https://github.com/recommenders-team/recommenders?tab=readme-ov-file)] [[2](https://github.com/AmazingDD/daisyRec)]  
DaisyRec 2.0: Benchmarking Recommendation for Rigorous Evaluation [[2022](https://arxiv.org/pdf/2206.10848)]

## temporarily store
Customized Multiple Clustering via Multi-Modal Subspace Proxy Learning  
Interactive Deep Clustering via Value Mining  

## Optimal Transportation
Unsupervised Learning of Visual Features by Contrasting Cluster Assignments [[NeurIPS 2020](https://arxiv.org/pdf/2006.09882)]  
Towards Interpretable Deep Metric Learning with Structural Matching [[ICCV 2021](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Towards_Interpretable_Deep_Metric_Learning_With_Structural_Matching_ICCV_2021_paper.pdf)]  
Optimal Transport Aggregation for Visual Place Recognition[[CVPR 2024](https://arxiv.org/abs/2311.15937)]  

## interpretable
SPARSE AUTOENCODERS FIND HIGHLY INTERPRETABLE FEATURES IN LANGUAGE MODELS [[2024](https://arxiv.org/pdf/2309.08600)]  
Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE )[[2024](https://arxiv.org/pdf/2402.10376)]  
ENHANCING PRE-TRAINED REPRESENTATION CLASSIFIABILITY CAN BOOST ITS INTERPRETABILITY[[under review 2025 ICLR](https://openreview.net/pdf?id=GjfIZan5jN)]  
